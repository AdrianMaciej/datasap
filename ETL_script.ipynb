{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c322230e-7fc8-43ba-a040-366d890874d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f6d04b50-0dad-458b-a7ae-a91628bdcd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_PARTITIONS = 2\n",
    "INPUT_LOC = 's3/somebucketname/tpcds-filtered'\n",
    "OUTPUT_LOC = 's3/somebucketname/tpcds-dwh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e4f26ec5-06c6-415e-a1bd-634fcb4fd44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import IntegerType, DecimalType, StringType, DateType, LongType, FloatType\n",
    "\n",
    "# initialise sparkContext\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local') \\\n",
    "    .appName('TPC-DS') \\\n",
    "    .config('spark.executor.memory', '5gb') \\\n",
    "    .config(\"spark.cores.max\", \"6\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", SHUFFLE_PARTITIONS) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def fill_na(df, fk_cols):\n",
    "    num_cols = [sf.name for sf in df.schema.fields if sf.name not in fk_cols and (isinstance(sf.dataType, IntegerType) or isinstance(sf.dataType, LongType))]\n",
    "    double_cols = [sf.name for sf in df.schema.fields if sf.name not in fk_cols and (isinstance(sf.dataType, DecimalType) or isinstance(sf.dataType, FloatType))]\n",
    "    str_cols = [sf.name for sf in df.schema.fields if sf.name not in fk_cols and isinstance(sf.dataType, StringType)]\n",
    "    date_cols = [sf.name for sf in df.schema.fields if sf.name not in fk_cols and isinstance(sf.dataType, DateType)]\n",
    "    # combine all na mappings\n",
    "    na_mapping = {\n",
    "        **{k:-1 for k in fk_cols}, \n",
    "        **{k:0 for k in num_cols}, \n",
    "        **{k:0.0 for k in double_cols}, \n",
    "        **{k:'UNKNOWN' for k in str_cols},\n",
    "        **{k:\"1990-01-01 00:00:00\" for k in date_cols}}\n",
    "    return df.na.fill(na_mapping)\n",
    "\n",
    "def get_valid_dim_rows(dim_df, fact_df, dim_col, fact_col):\n",
    "    # take only records that are in dimension and fact table\n",
    "    return dim_df.join(fact_df.select(sf.col(fact_col).alias(dim_col)).distinct(), dim_col, \"right_outer\")\n",
    "\n",
    "def get_invalid_dim_rows(dim_df, fact_df, dim_col, fact_col):\n",
    "    # take only records that are not_referenced in fact table\n",
    "    return dim_df.join(fact_df.select(sf.col(fact_col).alias(dim_col)).distinct(), dim_col, \"left_anti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ad232dc1-b17d-434f-bd8f-08d2d23ebbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "customer_raw = spark.read.parquet(f'{INPUT_LOC}/customer/')\n",
    "date_dim_raw = spark.read.parquet(f'{INPUT_LOC}/date_dim/')\n",
    "item_raw = spark.read.parquet(f'{INPUT_LOC}/item/')\n",
    "store_raw = spark.read.parquet(f'{INPUT_LOC}/store/')\n",
    "store_sales_raw = spark.read.parquet(f'{INPUT_LOC}/store_sales/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "09fb224c-d286-43b3-a287-3a43eccbb2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pk is unique\n",
    "def pk_check_log_and_exit(table):\n",
    "    print(f\"Table {table} doesn't have unique FK. Exiting.\")\n",
    "    \n",
    "if customer_raw.select('c_customer_sk').distinct().count() != customer_raw.count():\n",
    "    pk_check_log_and_exit(customer_raw)\n",
    "if date_dim_raw.select('d_date_id').distinct().count() != date_dim_raw.count():\n",
    "    pk_check_log_and_exit(date_dim_raw)\n",
    "if item_raw.select('i_item_sk').distinct().count() != item_raw.count():\n",
    "    pk_check_log_and_exit(item_raw)\n",
    "if store_raw.select('s_store_sk').distinct().count() != store_raw.count():\n",
    "    pk_check_log_and_exit(store_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "94cb557e-902c-4e80-9e8e-3cb4b0247610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null values in facts first - we need ids when joining with dimensions\n",
    "store_sales = fill_na(store_sales_raw, ['ss_customer_sk', 'ss_sold_date_sk', 'ss_item_sk', 'ss_store_sk'])\n",
    "\n",
    "# find valid dimensions - those that are in the fact table - including missing with -1\n",
    "customer_filtr = get_valid_dim_rows(customer_raw, store_sales, \"c_customer_sk\", \"ss_customer_sk\")\n",
    "date_dim_filtr = get_valid_dim_rows(date_dim_raw, store_sales, \"d_date_sk\", \"ss_sold_date_sk\")\n",
    "item_filtr = get_valid_dim_rows(item_raw, store_sales, \"i_item_sk\", \"ss_item_sk\")\n",
    "store_filtr = get_valid_dim_rows(store_raw, store_sales, \"s_store_sk\", \"ss_store_sk\")\n",
    "\n",
    "# find dimensions that are not in fact table for auditing\n",
    "customer_invalid = get_invalid_dim_rows(customer_raw, store_sales, \"c_customer_sk\", \"ss_customer_sk\")\n",
    "date_dim_invalid = get_invalid_dim_rows(date_dim_raw, store_sales, \"d_date_sk\", \"ss_sold_date_sk\")\n",
    "item_invalid = get_invalid_dim_rows(item_raw, store_sales, \"i_item_sk\", \"ss_item_sk\")\n",
    "store_invalid = get_invalid_dim_rows(store_raw, store_sales, \"s_store_sk\", \"ss_store_sk\")\n",
    "\n",
    "# fill null values in dimensions - since there might be -1 we need to do this after joining\n",
    "customer = fill_na(customer_filtr, ['c_customer_sk'])\n",
    "date_dim = fill_na(date_dim_filtr, ['d_date_sk'])\n",
    "item = fill_na(item_filtr, ['i_item_sk'])\n",
    "store = fill_na(store_filtr, ['s_store_sk'])\n",
    "\n",
    "# remove unnecessery keys\n",
    "store_sales = store_sales.drop('ss_sold_time_sk', 'ss_cdemo_sk', 'ss_hdemo_sk', 'ss_addr_sk', 'ss_promo_sk')\n",
    "customer = customer.drop('c_current_cdemo_sk', 'c_current_hdemo_sk', 'c_current_addr_sk')\n",
    "\n",
    "# custom dimensions\n",
    "def make_simple_dimension(source_df, on_column, as_column, as_column_id):\n",
    "    return source_df \\\n",
    "        .select(sf.col(on_column).alias(as_column)) \\\n",
    "        .distinct() \\\n",
    "        .withColumn(as_column_id,  sf.monotonically_increasing_id()) \\\n",
    "        .withColumn(as_column_id,  sf.when(sf.col(as_column) == 'UNKNOWN', -1).otherwise(sf.col(as_column_id)))\n",
    "\n",
    "item_category = make_simple_dimension(item, 'i_category', 'ic_category', 'ic_category_sk')\n",
    "store_city = make_simple_dimension(store, 's_city', 'sc_city', 'sc_city_sk')\n",
    "store_country = make_simple_dimension(store, 's_country', 'sc_country', 'sc_country_sk')\n",
    "\n",
    "# join dimensions to extract additonal data to fact table\n",
    "# get d_date for partitioning\n",
    "store_sales = store_sales.join(date_dim.select(sf.col('d_date_sk').alias('ss_sold_date_sk'), sf.col('d_date').alias('ss_d_date')), 'ss_sold_date_sk', 'left')\n",
    "item = item.join(item_category.select(sf.col('ic_category').alias('i_category'), sf.col('ic_category_sk').alias('i_category_sk')), 'i_category', 'left')\n",
    "store = store.join(store_city.select(sf.col('sc_city').alias('s_city'), sf.col('sc_city_sk').alias('s_city_sk')), 's_city', 'left')\n",
    "store = store.join(store_country.select(sf.col('sc_country').alias('s_country'), sf.col('sc_country_sk').alias('s_country_sk')), 's_country', 'left')\n",
    "# adding item_category id\n",
    "store_sales = store_sales.join(item.select(\n",
    "        sf.col('i_item_sk').alias('ss_item_sk'), \n",
    "        sf.col('i_category_sk').alias('ss_item_category_sk')), 'ss_item_sk', 'left')\n",
    "# adding store_city id and store_country id\n",
    "store_sales = store_sales.join(store.select(\n",
    "        sf.col('s_store_sk').alias('ss_store_sk'), \n",
    "        sf.col('s_city_sk').alias('ss_store_city_sk'), \n",
    "        sf.col('s_country_sk').alias('ss_store_country_sk')), 'ss_store_sk', 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dec2bb83-a355-4637-83ac-0664af488cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_store_sales_store_lvl\n",
    "## group facts by date and store\n",
    "## gives information about\n",
    "## - items sold, distinct items sold\n",
    "## - how many different customers per day\n",
    "## - how many purchases per day\n",
    "## - sum and avg of different prices\n",
    "groupBy = ['ss_d_date', 'ss_store_sk']\n",
    "cntFields = ['ss_item_sk']\n",
    "cntDistinctFields = ['ss_item_sk', 'ss_customer_sk', 'ss_ticket_number']\n",
    "sumFields = ['ss_item_sk', 'ss_quantity', 'ss_wholesale_cost', 'ss_list_price', 'ss_sales_price', 'ss_ext_discount_amt', 'ss_ext_list_price', 'ss_ext_tax', 'ss_coupon_amt', 'ss_net_paid' ,'ss_net_paid_inc_tax', 'ss_net_profit']\n",
    "avgFields = ['ss_item_sk', 'ss_quantity', 'ss_wholesale_cost', 'ss_list_price', 'ss_sales_price', 'ss_ext_discount_amt', 'ss_ext_list_price', 'ss_ext_tax', 'ss_coupon_amt', 'ss_net_paid' ,'ss_net_paid_inc_tax', 'ss_net_profit']\n",
    "aggregations = [sf.count(x).alias(f'count_{x}') for x in cntFields]\n",
    "aggregations = aggregations + [sf.countDistinct(x).alias(f'cnt_dist_{x}') for x in cntDistinctFields]\n",
    "aggregations = aggregations + [sf.sum(x).alias(f'sum_{x}') for x in sumFields]\n",
    "aggregations = aggregations + [sf.avg(x).alias(f'avg_{x}') for x in avgFields]\n",
    "\n",
    "a_store_sales_store_lvl = store_sales \\\n",
    "                            .groupBy(groupBy) \\\n",
    "                            .agg(*aggregations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4203c4d3-1591-459d-bd91-b2a3a80ac86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_store_sales_customer_lvl\n",
    "## group facts by date and customer\n",
    "## gives information about\n",
    "## - items bought, distinct items bought\n",
    "## - how many stores is he using\n",
    "## - number of purchases per day\n",
    "## - sum and avg of different prices\n",
    "groupBy = ['ss_d_date', 'ss_customer_sk']\n",
    "cntFields = ['ss_item_sk']\n",
    "cntDistinctFields = ['ss_item_sk', 'ss_store_sk', 'ss_ticket_number']\n",
    "sumFields = ['ss_item_sk', 'ss_quantity', 'ss_wholesale_cost', 'ss_list_price', 'ss_sales_price', 'ss_ext_discount_amt', 'ss_ext_list_price', 'ss_ext_tax', 'ss_coupon_amt', 'ss_net_paid' ,'ss_net_paid_inc_tax', 'ss_net_profit']\n",
    "avgFields = ['ss_item_sk', 'ss_quantity', 'ss_wholesale_cost', 'ss_list_price', 'ss_sales_price', 'ss_ext_discount_amt', 'ss_ext_list_price', 'ss_ext_tax', 'ss_coupon_amt', 'ss_net_paid' ,'ss_net_paid_inc_tax', 'ss_net_profit']\n",
    "aggregations = [sf.count(x).alias(f'count_{x}') for x in cntFields]\n",
    "aggregations = aggregations + [sf.countDistinct(x).alias(f'cnt_dist_{x}') for x in cntDistinctFields]\n",
    "aggregations = aggregations + [sf.sum(x).alias(f'sum_{x}') for x in sumFields]\n",
    "aggregations = aggregations + [sf.avg(x).alias(f'avg_{x}') for x in avgFields]\n",
    "\n",
    "a_store_sales_customer_lvl = store_sales \\\n",
    "                            .groupBy(groupBy) \\\n",
    "                            .agg(*aggregations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e710830a-3e62-4a5a-833d-2fc90e150b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_store_sales_item_lvl\n",
    "## group facts by date and item\n",
    "## gives information about\n",
    "## - items sold per day\n",
    "## - how many different customers per day\n",
    "## - how many purchases per day\n",
    "## - sum and avg of different prices\n",
    "groupBy = ['ss_d_date', 'ss_item_sk']\n",
    "cntFields = ['ss_item_sk']\n",
    "cntDistinctFields = ['ss_store_sk', 'ss_customer_sk', 'ss_ticket_number']\n",
    "sumFields = ['ss_item_sk', 'ss_quantity', 'ss_wholesale_cost', 'ss_list_price', 'ss_sales_price', 'ss_ext_discount_amt', 'ss_ext_list_price', 'ss_ext_tax', 'ss_coupon_amt', 'ss_net_paid' ,'ss_net_paid_inc_tax', 'ss_net_profit']\n",
    "avgFields = ['ss_item_sk', 'ss_quantity', 'ss_wholesale_cost', 'ss_list_price', 'ss_sales_price', 'ss_ext_discount_amt', 'ss_ext_list_price', 'ss_ext_tax', 'ss_coupon_amt', 'ss_net_paid' ,'ss_net_paid_inc_tax', 'ss_net_profit']\n",
    "aggregations = [sf.count(x).alias(f'count_{x}') for x in cntFields]\n",
    "aggregations = aggregations + [sf.countDistinct(x).alias(f'cnt_dist_{x}') for x in cntDistinctFields]\n",
    "aggregations = aggregations + [sf.sum(x).alias(f'sum_{x}') for x in sumFields]\n",
    "aggregations = aggregations + [sf.avg(x).alias(f'avg_{x}') for x in avgFields]\n",
    "\n",
    "a_store_sales_item_lvl = store_sales \\\n",
    "                            .groupBy(groupBy) \\\n",
    "                            .agg(*aggregations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "24525ea6-eab5-4929-af0c-8dbe5c0881dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_store_sales_store_quantity_ma\n",
    "## calculates moving average of each store for last 7, 30, 90 days\n",
    "\n",
    "# find min and max date for each store\n",
    "min_max_date_store = a_store_sales_store_lvl.groupBy('ss_store_sk').agg(sf.min('ss_d_date').alias('min_d_date'), sf.max('ss_d_date').alias('max_d_date'))\n",
    "# cross join will all dates\n",
    "min_max_date_store = min_max_date_store.crossJoin(date_dim_raw.select(sf.col('d_date').alias('ss_d_date')))\n",
    "# filter lower than min and higher than max \n",
    "min_max_date_store = min_max_date_store.filter((sf.col('ss_d_date') >= sf.col('min_d_date')) & (sf.col('ss_d_date') <= sf.col('max_d_date')))\n",
    "# take only store and date cols\n",
    "min_max_date_store = min_max_date_store.select('ss_store_sk', 'ss_d_date')\n",
    "\n",
    "# join facts will all valid dates - fill missing values with 0\n",
    "a_store_sales_store_quantity_ma = min_max_date_store \\\n",
    "    .join(a_store_sales_store_lvl, ['ss_store_sk', 'ss_d_date'], 'left_outer') \\\n",
    "    .fillna(0) \\\n",
    "    .select('ss_d_date', 'ss_store_sk', 'avg_ss_quantity')\n",
    "\n",
    "# get 7d MA\n",
    "a_store_sales_store_quantity_ma = a_store_sales_store_quantity_ma \\\n",
    "    .withColumn(\"7d_ma_quantity\", \n",
    "                sf.avg(\"avg_ss_quantity\").over(Window.partitionBy('ss_store_sk').orderBy('ss_d_date').rowsBetween(-7, Window.currentRow)))\n",
    "# get 1m MA\n",
    "a_store_sales_store_quantity_ma = a_store_sales_store_quantity_ma \\\n",
    "    .withColumn(\"1m_ma_quantity\", \n",
    "                sf.avg(\"avg_ss_quantity\").over(Window.partitionBy('ss_store_sk').orderBy('ss_d_date').rowsBetween(-30, Window.currentRow)))\n",
    "# get 1y MA\n",
    "a_store_sales_store_quantity_ma = a_store_sales_store_quantity_ma \\\n",
    "    .withColumn(\"1q_ma_quantity\", \n",
    "                sf.avg(\"avg_ss_quantity\").over(Window.partitionBy('ss_store_sk').orderBy('ss_d_date').rowsBetween(-90, Window.currentRow)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8cdd38b7-444e-4607-ad1e-90c7e415691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_as_parquet(df, loc, part_by = None):\n",
    "    if part_by:\n",
    "        df.write.partitionBy(part_by).mode(\"overwrite\").parquet(loc)\n",
    "    else:\n",
    "        df.write.mode(\"overwrite\").parquet(loc)\n",
    "    \n",
    "# fact and dimensions\n",
    "write_as_parquet(customer, f'{OUTPUT_LOC}/d_customer/')\n",
    "write_as_parquet(date_dim, f'{OUTPUT_LOC}/d_date_dim/')\n",
    "write_as_parquet(item, f'{OUTPUT_LOC}/d_item/')\n",
    "write_as_parquet(store, f'{OUTPUT_LOC}/d_store/')\n",
    "write_as_parquet(item_category, f'{OUTPUT_LOC}/d_item_category/')\n",
    "write_as_parquet(store_city, f'{OUTPUT_LOC}/d_store_city/')\n",
    "write_as_parquet(store_country, f'{OUTPUT_LOC}/d_store_country/')\n",
    "write_as_parquet(store_sales, f'{OUTPUT_LOC}/f_store_sales/')\n",
    "# given full dataset it would make sense to partition by d_date for example\n",
    "#write_as_parquet(store_sales, 'tpcds-dwh/store_sales_part/', 'd_date')\n",
    "\n",
    "# audit\n",
    "write_as_parquet(customer_invalid, f'{OUTPUT_LOC}/d_customer_invalid/')\n",
    "write_as_parquet(date_dim_invalid, f'{OUTPUT_LOC}/d_date_dim_invalid/')\n",
    "write_as_parquet(item_invalid, f'{OUTPUT_LOC}/d_item_invalid/')\n",
    "write_as_parquet(store_invalid, f'{OUTPUT_LOC}/d_store_invalid/')\n",
    "\n",
    "# aggregations\n",
    "write_as_parquet(a_store_sales_store_lvl, f'{OUTPUT_LOC}/a_store_sales_store_lvl/')\n",
    "write_as_parquet(a_store_sales_customer_lvl, f'{OUTPUT_LOC}/a_store_sales_customer_lvl/')\n",
    "write_as_parquet(a_store_sales_item_lvl, f'{OUTPUT_LOC}/a_store_sales_item_lvl/')\n",
    "\n",
    "# windowing\n",
    "write_as_parquet(a_store_sales_store_quantity_ma, f'{OUTPUT_LOC}/a_store_sales_store_quantity_ma/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
